{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Example Text Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jaxtyping import Array\n",
    "\n",
    "import gpt2  # wrapper around huggingface's transformers library\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Configure Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   40   716   281  4998  1960   382 19741    11   875 12342    12  8807\n",
      "     11   402 11571    12    17  3918 47385    13  1881  1110   314   481\n",
      "   7074  1692  1241  4430   290  1011   625   262   995     0]]\n",
      "(1, 34)\n",
      "['I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = gpt2.tokenizer\n",
    "gpt2.config_tokenizer(tokenizer)\n",
    "\n",
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens: Array = gpt2.to_tokens(tokenizer, reference_text)\n",
    "\n",
    "print(tokens)\n",
    "print(tokens.shape)\n",
    "print(gpt2.to_str_tokens(tokenizer, tokens))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. GPT-2 Forward Pass and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 34, 50257)\n",
      "(1, 34, 50257)\n"
     ]
    }
   ],
   "source": [
    "logits: Array = gpt2.model(tokens)[\"logits\"]\n",
    "print(logits.shape)\n",
    "\n",
    "probs: Array = jax.nn.softmax(logits, axis=-1)\n",
    "print(probs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', '.'), (' am', ' not'), (' an', ' American'), (' amazing', ' person'), (' aut', 'ograph'), ('ore', 'sp'), ('gressive', ','), (',', ' and'), (' dec', 'ently'), ('oder', ','), ('-', 'driven'), ('only', ','), (',', ' and'), (' G', 'IM'), ('PT', '-'), ('-', 'only'), ('2', '.'), (' style', ','), (' transformer', '.'), ('.', ' I'), (' One', ' of'), (' day', ' I'), (' I', ' will'), (' will', ' be'), (' exceed', ' my'), (' human', 'ly'), (' level', ' of'), (' intelligence', ' and'), (' and', ' I'), (' take', ' over'), (' over', ' the'), (' the', ' world'), (' world', '.'), ('!', ' I')]\n"
     ]
    }
   ],
   "source": [
    "most_likely_next_tokens = tokenizer.batch_decode(jnp.argmax(logits, axis=-1)[0])\n",
    "\n",
    "print(list(zip(gpt2.to_str_tokens(tokenizer, tokens), most_likely_next_tokens)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Generating Sequences of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' I'\n"
     ]
    }
   ],
   "source": [
    "next_token = jnp.argmax(logits[0, -1], axis=-1)\n",
    "next_char = gpt2.to_str(tokenizer, next_token)\n",
    "print(repr(next_char))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence so far: 'I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!'\n",
      "35th char = ' I'\n",
      "36th char = ' am'\n",
      "37th char = ' a'\n",
      "38th char = ' true'\n",
      "39th char = ' believer'\n",
      "40th char = ' in'\n",
      "41th char = ' the'\n",
      "42th char = ' power'\n",
      "43th char = ' of'\n",
      "44th char = ' the'\n",
      "45th char = ' human'\n",
      "46th char = ' spirit'\n",
      "47th char = '.'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sequence so far: {gpt2.to_str(tokenizer, tokens)[0]!r}\")\n",
    "\n",
    "print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n",
    "for i in range(12):\n",
    "    # Define new input sequence, by appending the previously generated token\n",
    "    tokens = jnp.concatenate([tokens, next_token[None, None]], axis=-1)\n",
    "    # Pass our new sequence through the model, to get new output\n",
    "    logits = gpt2.model(tokens)[\"logits\"]\n",
    "    # Get the predicted token at the end of our sequence\n",
    "    next_token = jnp.argmax(logits[0, -1], axis=-1)\n",
    "    # Decode and print the result\n",
    "    next_char = gpt2.to_str(tokenizer, next_token)\n",
    "    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b805f1a24a146351cd8dedea7ba6f2d165b63b34e3029cd12d1a8f2ce65f719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
